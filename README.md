# ğŸš€ Advanced AI Website Chatbot

## ğŸ“– Project Overview

The **Advanced AI Website Chatbot** is a sophisticated web scraping and conversational AI application that combines cutting-edge technologies to extract, process, and interact with website content. This project enables users to scrape entire websites, extract text from images using OCR, store data persistently, and chat with an AI assistant about the collected information using voice or text input.

### ğŸ¯ Key Features

- **ğŸ” Intelligent Web Scraping**: Automated discovery and scraping of website pages with parallel processing
- **ğŸ–¼ï¸ Image OCR Technology**: Extracts text from images using both EasyOCR (GPU) and Tesseract (CPU)
- **ğŸ¤ Voice Interaction**: Full voice-to-text and text-to-speech capabilities for hands-free operation  
- **ğŸ’¾ Persistent Storage**: ChromaDB vector database for permanent content storage and retrieval
- **ğŸ¤– AI-Powered Responses**: Google Gemini integration for intelligent question answering
- **âš¡ Parallel Processing**: Multi-threaded scraping for enhanced performance
- **ğŸŒ Smart URL Discovery**: Intelligent identification of important website pages

## ğŸ—ï¸ System Architecture

### Core Components

1. **Frontend Interface** (Streamlit)
   - Interactive web application
   - Real-time configuration panel
   - Chat interface with history
   - Voice interaction controls

2. **Web Scraping Engine** (AdvancedWebsiteScraper)
   - Selenium WebDriver automation
   - BeautifulSoup HTML parsing
   - Parallel page processing
   - Smart URL discovery algorithm

3. **OCR Processing System**
   - EasyOCR for GPU acceleration
   - Tesseract for CPU processing
   - Image text extraction pipeline
   - Multi-format image support

4. **Vector Storage Layer** (ChromaDB)
   - Persistent content storage
   - Semantic search capabilities
   - Document chunking strategy
   - Metadata preservation

5. **AI Response Generation** (Google Gemini)
   - Context-aware responses
   - Source attribution
   - Natural language processing
   - Query understanding

6. **Voice Processing Module**
   - Speech Recognition (Google Speech API)
   - Text-to-Speech (gTTS)
   - Real-time audio processing
   - Voice command handling

### ğŸ”„ Data Flow Architecture

```
Website URL Input â†’ Smart URL Discovery â†’ Parallel Web Scraping â†’ Content Extraction
                                                                         â†“
OCR Text Extraction â† Image Processing â† HTML Content Parsing â† Page Loading
         â†“
Content Chunking â†’ Vector Embeddings â†’ ChromaDB Storage â†’ Persistent Database
                                                               â†“
User Query (Text/Voice) â†’ Vector Search â†’ Context Retrieval â†’ AI Response Generation
                                                                      â†“
Response Display â†’ Voice Synthesis â†’ Audio Playback â†’ Source Citations
```

## ğŸ› ï¸ Technical Implementation

### Web Scraping Pipeline

The scraping system implements a sophisticated multi-stage process:

1. **URL Discovery Phase**
   - Intelligently crawls website structure
   - Prioritizes important pages (/about, /services, /products, etc.)
   - Validates URLs and filters invalid content types
   - Respects domain boundaries

2. **Parallel Processing Engine**
   - Configurable worker threads (1-5 workers)
   - Chrome WebDriver pool management
   - Progress tracking and error handling
   - Resource optimization strategies

3. **Content Extraction Methods**
   - BeautifulSoup HTML parsing
   - Unwanted element removal (ads, scripts, navigation)
   - Text normalization and cleaning
   - Title and metadata extraction

### OCR Technology Integration

The system supports dual OCR engines for maximum compatibility:

**EasyOCR (GPU Mode)**
- Deep learning-based text recognition
- Support for multiple languages
- High accuracy for complex layouts
- GPU acceleration for speed

**Tesseract (CPU Mode)**
- Traditional OCR engine
- Lightweight processing
- Fallback option for systems without GPU
- Reliable text extraction

### Vector Storage System

ChromaDB implementation provides:

- **Persistent Storage**: Data survives application restarts
- **Semantic Search**: Vector-based content similarity matching
- **Chunking Strategy**: 800-word segments for optimal retrieval
- **Metadata Preservation**: URL, title, image count, timestamps
- **Collection Management**: Organized content categorization

### AI Response Generation

Google Gemini integration features:

- **Context-Aware Processing**: Uses retrieved content as context
- **Source Attribution**: References specific content sources
- **Natural Language Understanding**: Interprets complex queries
- **Conversational Memory**: Maintains chat history

## ğŸ“ Project Structure

```
advanced_chatbot.py
â”œâ”€â”€ Imports & Dependencies
â”œâ”€â”€ Class: AdvancedWebsiteScraper
â”‚   â”œâ”€â”€ Driver Management
â”‚   â”œâ”€â”€ OCR Setup & Processing  
â”‚   â”œâ”€â”€ URL Validation & Discovery
â”‚   â”œâ”€â”€ Single Page Scraping
â”‚   â””â”€â”€ Parallel Processing
â”œâ”€â”€ Class: VoiceInteraction
â”‚   â”œâ”€â”€ Speech Recognition
â”‚   â””â”€â”€ Text-to-Speech
â”œâ”€â”€ Class: PersistentVectorStorage
â”‚   â”œâ”€â”€ ChromaDB Management
â”‚   â”œâ”€â”€ Content Storage
â”‚   â”œâ”€â”€ Vector Search
â”‚   â””â”€â”€ Statistics
â”œâ”€â”€ AI Response Functions
â”‚   â””â”€â”€ Gemini Integration
â””â”€â”€ Main Streamlit Application
    â”œâ”€â”€ Configuration Sidebar
    â”œâ”€â”€ Scraping Interface  
    â”œâ”€â”€ Voice Controls
    â”œâ”€â”€ Chat Interface
    â””â”€â”€ Content Display
```

## ğŸ’» Dependencies & Requirements

### Core Libraries

| Library | Purpose | Version |
|---------|---------|---------|
| `streamlit` | Web application framework | Latest |
| `selenium` | Web browser automation | Latest |
| `beautifulsoup4` | HTML parsing | Latest |
| `google-generativeai` | AI response generation | Latest |
| `sentence-transformers` | Text embeddings | Latest |
| `chromadb` | Vector database | Latest |
| `easyocr` | GPU-based OCR | Latest |
| `pytesseract` | CPU-based OCR | Latest |
| `speech-recognition` | Voice input processing | Latest |
| `gtts` | Text-to-speech | Latest |
| `requests` | HTTP requests | Latest |
| `torch` | Machine learning backend | Latest |
| `PIL` | Image processing | Latest |
| `python-dotenv` | Environment variables | Latest |
| `streamlit-webrtc` | Real-time audio processing | Latest |

### System Requirements

- **Operating System**: Windows, macOS, or Linux
- **Python Version**: 3.8 or higher
- **RAM**: Minimum 4GB (8GB recommended)
- **Storage**: 2GB free space
- **GPU**: Optional (for EasyOCR acceleration)
- **Chrome Browser**: Required for web scraping

## ğŸš€ Installation Guide

### Step 1: Environment Setup

```bash
# Create virtual environment
python -m venv chatbot_env

# Activate environment
# Windows:
chatbot_env\Scripts\activate
# macOS/Linux:
source chatbot_env/bin/activate
```

### Step 2: Install Dependencies

```bash
pip install streamlit
pip install selenium
pip install beautifulsoup4
pip install google-generativeai
pip install sentence-transformers
pip install chromadb
pip install easyocr
pip install pytesseract
pip install SpeechRecognition
pip install gtts
pip install requests
pip install torch
pip install Pillow
pip install python-dotenv
pip install streamlit-webrtc
```

### Step 3: Chrome Driver Setup

Download ChromeDriver from https://chromedriver.chromium.org/ and add to system PATH.

### Step 4: Environment Configuration

Create `.env` file:
```
GEMINI_API_KEY=your_api_key_here
```

## âš™ï¸ Configuration Options

### Scraping Settings

- **Max Pages**: 1-50 pages per website (default: 15)
- **Parallel Workers**: 1-5 concurrent scrapers (default: 3)
- **Image OCR**: Enable/disable text extraction from images
- **GPU OCR**: Use EasyOCR (GPU) vs Tesseract (CPU)

### Voice Settings

- **Voice Responses**: Enable AI voice output
- **Speech Recognition**: Google Speech API integration
- **Audio Format**: MP3 with gTTS synthesis

### Storage Settings

- **Collection Name**: Customize database collection
- **Persist Directory**: ChromaDB storage location
- **Chunk Size**: 800 words per content segment

## ğŸ® Usage Instructions

### Basic Workflow

1. **Launch Application**
   ```bash
   streamlit run advanced_chatbot.py
   ```

2. **Configure API Key**
   - Enter Google Gemini API key in sidebar
   - Obtain key from Google AI Studio

3. **Set Scraping Parameters**
   - Adjust max pages and workers
   - Enable OCR and voice features
   - Configure collection name

4. **Scrape Website**
   - Enter target website URL
   - Click "Start Advanced Scraping"
   - Monitor progress indicators

5. **Interact with Content**
   - Use text chat interface
   - Enable voice queries
   - Review source citations

### Advanced Features

**Voice Interaction Workflow**
1. Enable "Voice Responses" in sidebar
2. Click "Ask Question by Voice"
3. Speak clearly within 2 seconds
4. Wait for speech processing
5. Receive text and audio response

**OCR Text Extraction**
- Automatically processes first 5 images per page
- Extracts text using AI-powered recognition
- Integrates image text with page content
- Displays image count in results

**Persistent Storage Benefits**
- Content survives application restarts
- No need to re-scrape websites
- Fast semantic search across stored data
- Maintains chat history and context

## ğŸ” How It Works

### 1. Intelligent URL Discovery

The system starts with a base URL and intelligently discovers related pages:

```python
# Priority patterns for important pages
priority_patterns = [
    '/about', '/services', '/products', '/blog', '/news', '/faq',
    '/contact', '/help', '/docs', '/documentation', '/api'
]
```

The crawler prioritizes these important sections while respecting domain boundaries and filtering out unwanted file types.

### 2. Parallel Web Scraping

Multiple Chrome browser instances work simultaneously:

```python
# Parallel processing with progress tracking
with ThreadPoolExecutor(max_workers=max_workers) as executor:
    future_to_url = {
        executor.submit(self.scrape_single_page, url, extract_images): url
        for url in urls
    }
```

Each worker:
- Opens a headless Chrome browser
- Loads the target page
- Extracts HTML content
- Processes images for OCR text
- Returns structured data

### 3. OCR Text Extraction

For each page, the system identifies images and extracts text:

```python
# EasyOCR processing
results = self.ocr_reader.readtext(response.content)
text = " ".join([result[1] for result in results])

# Tesseract fallback
text = pytesseract.image_to_string(pil_img)
```

This dual approach ensures compatibility across different hardware configurations.

### 4. Vector Storage and Retrieval

Content is chunked and stored with metadata:

```python
# Content chunking for better search
chunk_size = 800
words = content.split()
for j, chunk_start in enumerate(range(0, len(words), chunk_size)):
    chunk = ' '.join(words[chunk_start:chunk_start + chunk_size])
```

ChromaDB handles vector embeddings and semantic search automatically.

### 5. AI Response Generation

Retrieved content provides context for AI responses:

```python
# Prepare context from search results
context = ""
for i, item in enumerate(relevant_content):
    metadata = item.get('metadata', {})
    content = item.get('content', '')
    context += f"\n\nSource {i+1} (from {metadata.get('title', 'Unknown')}):\n{content}"
```

Google Gemini uses this context to generate accurate, source-attributed responses.

## ğŸ¯ Use Cases

### 1. Research and Analysis
- Academic research on specific topics
- Market analysis and competitor research  
- Technical documentation exploration
- News and current events tracking

### 2. Business Intelligence
- Company website analysis
- Product information extraction
- Service offering comparisons
- Contact and location data gathering

### 3. Content Accessibility
- Voice-enabled browsing for accessibility
- Image text extraction for visually impaired users
- Audio summaries of web content
- Multilingual content processing

### 4. Educational Applications
- Course material extraction and summarization
- Interactive learning with voice queries
- Research paper analysis and discussion
- Study guide generation from online resources

## ğŸ”’ Security & Privacy

### Data Handling
- All scraped content stored locally in ChromaDB
- No data transmitted to external servers (except AI API calls)
- User control over data retention and deletion
- Secure API key management through environment variables

### Web Scraping Ethics
- Respects robots.txt and website policies
- Implements rate limiting and delays
- Uses standard user agents
- Focuses on publicly available content

## ğŸš§ Troubleshooting

### Common Issues

**Chrome Driver Errors**
```
Solution: Ensure ChromeDriver is installed and in PATH
Download from: https://chromedriver.chromium.org/
```

**OCR Processing Failures**
```
Solution: Install Tesseract OCR system dependency
Windows: Download installer from GitHub
macOS: brew install tesseract
Linux: apt-get install tesseract-ocr
```

**Voice Recognition Issues**
```
Solution: Check microphone permissions and internet connection
Ensure PyAudio is properly installed for speech recognition
```

**Memory Issues with Large Websites**
```
Solution: Reduce max_pages and max_workers settings
Monitor system resource usage during scraping
```

## ğŸ”„ Future Enhancements

### Planned Features
- Support for additional file formats (PDF, DOCX)
- Multi-language OCR and voice processing
- Advanced content filtering and categorization
- Integration with additional AI models
- Export functionality for scraped data
- Real-time website monitoring and updates
- API endpoint for programmatic access

### Performance Improvements
- Caching mechanisms for faster repeated access
- Database optimization for large content volumes
- Asynchronous processing for better responsiveness
- Memory management enhancements

## ğŸ“Š Performance Metrics

### Typical Performance
- **Scraping Speed**: 3-5 pages per minute per worker
- **OCR Processing**: 2-3 images per second
- **Voice Recognition**: 1-2 second response time
- **AI Response Generation**: 3-5 seconds per query
- **Storage Efficiency**: ~1KB per content chunk

### Scalability
- **Concurrent Users**: Designed for single-user operation
- **Content Volume**: Handles 10,000+ pages efficiently  
- **Memory Usage**: ~500MB baseline, scales with content
- **Storage Requirements**: ~1MB per 100 pages scraped

## ğŸ¤ Contributing

This project welcomes contributions! Areas for improvement:
- Additional OCR engines and languages
- Enhanced voice processing capabilities
- Better error handling and recovery
- Performance optimizations
- UI/UX improvements
- Documentation enhancements

## ğŸ“„ License

This project is available for educational and research purposes. Please ensure compliance with website terms of service and applicable laws when scraping content.

## ğŸ†˜ Support

For issues and questions:
1. Check the troubleshooting section above
2. Review system requirements and installation steps
3. Ensure all dependencies are properly installed
4. Verify API keys and configuration settings

---

**Version**: 1.0
**Last Updated**: September 2025
**Compatibility**: Python 3.8+, Windows/macOS/Linux

This comprehensive documentation covers all aspects of the Advanced AI Website Chatbot project, from architecture to implementation details.
# Ask-Web
AskWeb is an AI-powered assistant that scrapes websites, processes text and images, and lets you ask questions via text or voice with instant answers powered by Google Gemini.
